{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d732dc9-e518-4dd5-82b4-fb6bea5c6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b140526-a3ea-49d7-b19d-ab5e71c17b76",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skimage'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hog\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'skimage'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load images from dataset directory\n",
    "def load_images_from_folder(folder, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in glob.glob(os.path.join(folder, \"*.jpg\")):  # Adjust extension if needed\n",
    "        img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)  # Convert to grayscale\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (64, 64))  # Resize to a fixed size\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "# Load dataset (Modify path according to your dataset)\n",
    "cat_images, cat_labels = load_images_from_folder(\"dataset/cats\", label=0)\n",
    "dog_images, dog_labels = load_images_from_folder(\"dataset/dogs\", label=1)\n",
    "\n",
    "# Combine dataset\n",
    "X = np.array(cat_images + dog_images)\n",
    "y = np.array(cat_labels + dog_labels)\n",
    "\n",
    "# Extract HOG features for each image\n",
    "hog_features = []\n",
    "for image in X:\n",
    "    feature, _ = hog(image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
    "    hog_features.append(feature)\n",
    "\n",
    "X_hog = np.array(hog_features)\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_hog, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an SVM classifier\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529d4828-6fbd-40a2-907e-de6f2c1e2e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-image\n",
      "  Downloading scikit_image-0.25.2-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.24 in c:\\python38-32\\lib\\site-packages (from scikit-image) (1.25.0)\n",
      "Requirement already satisfied: scipy>=1.11.4 in c:\\python38-32\\lib\\site-packages (from scikit-image) (1.12.0)\n",
      "Collecting networkx>=3.0 (from scikit-image)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting pillow>=10.1 (from scikit-image)\n",
      "  Downloading pillow-11.1.0-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting imageio!=2.35.0,>=2.33 (from scikit-image)\n",
      "  Using cached imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
      "  Using cached tifffile-2025.3.13-py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: packaging>=21 in c:\\python38-32\\lib\\site-packages (from scikit-image) (23.2)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Downloading scikit_image-0.25.2-cp311-cp311-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.8 MB 5.6 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 2.1/12.8 MB 6.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.7/12.8 MB 6.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.0/12.8 MB 6.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.6/12.8 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.9/12.8 MB 6.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.8 MB 6.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.7/12.8 MB 6.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.8 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 6.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 6.0 MB/s eta 0:00:00\n",
      "Using cached imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Downloading pillow-11.1.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 1.3/2.6 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.4/2.6 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 4.9 MB/s eta 0:00:00\n",
      "Using cached tifffile-2025.3.13-py3-none-any.whl (226 kB)\n",
      "Installing collected packages: tifffile, pillow, networkx, lazy-loader, imageio, scikit-image\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.5.0\n",
      "    Uninstalling Pillow-9.5.0:\n",
      "      Successfully uninstalled Pillow-9.5.0\n",
      "Successfully installed imageio-2.37.0 lazy-loader-0.4 networkx-3.4.2 pillow-11.1.0 scikit-image-0.25.2 tifffile-2025.3.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Python38-32\\Lib\\site-packages\\~il'.\n",
      "  You can safely remove it manually.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: C:\\Python38-32\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0f407-5386-4c60-a48b-db39c148d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_matrix = cv2.getRotationMatrix2D((num_cols/2, num_rows/2), 30, 1)\n",
    "test_image = cv2.warpAffine(test_image, rotation_matrix, (num_cols, num_rows))\n",
    "\n",
    "test_gray = cv2.cvtColor(test_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Display traning image and testing image\n",
    "fx, plots = plt.subplots(1, 2, figsize=(20,10))\n",
    "\n",
    "plots[0].set_title(\"Training Image\")\n",
    "plots[0].imshow(training_image)\n",
    "\n",
    "plots[1].set_title(\"Testing Image\")\n",
    "plots[1].imshow(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127391c4-4328-496d-b25e-cc5b51f72b2e",
   "metadata": {},
   "source": [
    "# Detect keypoints and Create Descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0b748-53a9-4b1d-9e69-7be287cb7ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Detect keypoints and compute descriptors for both images\n",
    "keypoints_train, descriptors_train = sift.detectAndCompute(training_gray, None)\n",
    "keypoints_test, descriptors_test = sift.detectAndCompute(test_gray, None)\n",
    "\n",
    "# Use BFMatcher to match descriptors\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "matches = bf.knnMatch(descriptors_train, descriptors_test, k=2)\n",
    "\n",
    "# Apply ratio test to filter matches\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# If sufficient matches are found, proceed with homography\n",
    "if len(good_matches) > 10:\n",
    "    # Extract matching keypoints' coordinates\n",
    "    src_pts = np.float32([keypoints_train[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints_test[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Compute homography matrix\n",
    "    matrix, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "    # Draw bounding box around detected object in the test image\n",
    "    h, w = training_gray.shape\n",
    "    corners = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1, 1, 2)\n",
    "    transformed_corners = cv2.perspectiveTransform(corners, matrix)\n",
    "\n",
    "    test_image_with_box = test_image.copy()\n",
    "    test_image_with_box = cv2.polylines(test_image_with_box,\n",
    "                                        [np.int32(transformed_corners)],\n",
    "                                        isClosed=True,\n",
    "                                        color=(255, 0, 0),\n",
    "                                        thickness=3)\n",
    "\n",
    "    # Display results\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.title(\"Detected Object\")\n",
    "    plt.imshow(test_image_with_box)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough matches found to detect the object.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba94bc-3782-4a39-8650-9bc8ca07d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "# Create Brute Force Matcher with L2 norm\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "\n",
    "# Perform KNN matching (find the two best matches for each descriptor)\n",
    "matches = bf.knnMatch(train_descriptor, test_descriptor, k=2)\n",
    "\n",
    "# Apply Loweâ€™s Ratio Test\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Sort matches based on distance\n",
    "good_matches = sorted(good_matches, key=lambda x: x.distance)\n",
    "\n",
    "# Find homography if enough matches are found\n",
    "if len(good_matches) > 4:\n",
    "    # Extract matching keypoints\n",
    "    src_pts = np.float32([train_keypoints[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([test_keypoints[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Compute homography using RANSAC\n",
    "    H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "    # Get object bounding box in training image\n",
    "    h, w = training_gray.shape\n",
    "    obj_corners = np.float32([[0, 0], [w, 0], [w, h], [0, h]]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Project bounding box onto test image\n",
    "    if H is not None:\n",
    "        scene_corners = cv2.perspectiveTransform(obj_corners, H)\n",
    "        \n",
    "        # Draw bounding box\n",
    "        test_image_color = cv2.cvtColor(test_gray, cv2.COLOR_GRAY2BGR)\n",
    "        cv2.polylines(test_image_color, [np.int32(scene_corners)], True, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "        # Draw matches\n",
    "        result = cv2.drawMatches(training_image, train_keypoints, test_image_color, test_keypoints,\n",
    "                                 good_matches[:50], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "        \n",
    "        # Display result\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        plt.title(\"Feature Matching & Object Detection\")\n",
    "        plt.imshow(result)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\nNumber of Filtered Matching Keypoints:\", len(good_matches))\n",
    "    else:\n",
    "        print(\"Homography could not be computed.\")\n",
    "else:\n",
    "    print(\"Not enough matches found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5987d10-f42d-4ef5-995f-0e825caa4d1e",
   "metadata": {},
   "source": [
    "# Matching Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e134b4-7d4d-4d1c-b0ab-1bd0d442acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BFMatcher object\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False)\n",
    "\n",
    "# Find matches using knn\n",
    "matches = bf.knnMatch(train_descriptor, test_descriptor, k=2)\n",
    "\n",
    "# Apply Lowe's ratio test\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good_matches.append(m)\n",
    "\n",
    "# Extract matched keypoints\n",
    "train_pts = np.float32([train_keypoints[m.queryIdx].pt for m in good_matches])\n",
    "\n",
    "# Apply KMeans to cluster keypoints into 4 groups (corresponding to 4 objects)\n",
    "num_objects = 4\n",
    "kmeans = KMeans(n_clusters=num_objects, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(train_pts)\n",
    "\n",
    "# Draw bounding boxes around clusters\n",
    "train_image_copy = training_image.copy()\n",
    "for i in range(num_objects):\n",
    "    cluster_points = train_pts[labels == i]\n",
    "    \n",
    "    if len(cluster_points) < 5:  # Ignore very small clusters (noise)\n",
    "        continue\n",
    "\n",
    "    x_min, y_min = np.min(cluster_points, axis=0)\n",
    "    x_max, y_max = np.max(cluster_points, axis=0)\n",
    "\n",
    "    # Expand bounding box slightly\n",
    "    padding = 20\n",
    "    x_min, y_min = max(0, x_min - padding), max(0, y_min - padding)\n",
    "    x_max, y_max = min(train_image_copy.shape[1], x_max + padding), min(train_image_copy.shape[0], y_max + padding)\n",
    "\n",
    "    # Draw bounding box\n",
    "    cv2.rectangle(train_image_copy, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 255, 0), 3)\n",
    "\n",
    "# Display detected objects\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(cv2.cvtColor(train_image_copy, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Detected Objects in Training Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d55b9a1-a81f-42cd-83b1-145345112603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detect_and_match_objects(reference_image_path, query_image_path, min_match_count=10):\n",
    "    \"\"\"\n",
    "    Detect objects in a query image based on a reference image using SIFT features.\n",
    "    \n",
    "    Parameters:\n",
    "    reference_image_path (str): Path to the reference image (object to be detected)\n",
    "    query_image_path (str): Path to the query image (where to find the object)\n",
    "    min_match_count (int): Minimum number of good matches required\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (result_image, homography_matrix, matches_mask)\n",
    "    \"\"\"\n",
    "    # Load images\n",
    "    reference_img = cv2.imread(reference_image_path)\n",
    "    query_img = cv2.imread(query_image_path)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    reference_gray = cv2.cvtColor(reference_img, cv2.COLOR_BGR2GRAY)\n",
    "    query_gray = cv2.cvtColor(query_img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Initialize SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "    \n",
    "    # Find keypoints and descriptors\n",
    "    kp_reference, des_reference = sift.detectAndCompute(reference_gray, None)\n",
    "    kp_query, des_query = sift.detectAndCompute(query_gray, None)\n",
    "    \n",
    "    # FLANN parameters for matching\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "    \n",
    "    # Create FLANN matcher\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    \n",
    "    # Match descriptors\n",
    "    matches = flann.knnMatch(des_reference, des_query, k=2)\n",
    "    \n",
    "    # Apply ratio test (Lowe's ratio test)\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            good_matches.append(m)\n",
    "    \n",
    "    # Create a mask for drawing matches\n",
    "    matches_mask = [[0, 0] for _ in range(len(matches))]\n",
    "    \n",
    "    # Try to find homography if we have enough good matches\n",
    "    homography = None\n",
    "    if len(good_matches) >= min_match_count:\n",
    "        # Extract location of good matches\n",
    "        src_pts = np.float32([kp_reference[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp_query[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        \n",
    "        # Find homography\n",
    "        homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "        \n",
    "        # Update matches_mask based on inliers\n",
    "        for i, (m, _) in enumerate(matches):\n",
    "            if m in good_matches and mask[good_matches.index(m)]:\n",
    "                matches_mask[i] = [1, 0]\n",
    "        \n",
    "        # Draw bounding box around the detected object\n",
    "        h, w = reference_gray.shape\n",
    "        corners = np.float32([[0, 0], [0, h-1], [w-1, h-1], [w-1, 0]]).reshape(-1, 1, 2)\n",
    "        transformed_corners = cv2.perspectiveTransform(corners, homography)\n",
    "        \n",
    "        # Draw the bounding box\n",
    "        result_img = query_img.copy()\n",
    "        cv2.polylines(result_img, [np.int32(transformed_corners)], True, (0, 255, 0), 3)\n",
    "        \n",
    "        print(f\"Object found - {len(good_matches)} good matches\")\n",
    "    else:\n",
    "        print(f\"Not enough matches found - {len(good_matches)}/{min_match_count}\")\n",
    "        result_img = query_img.copy()\n",
    "        homography = None\n",
    "    \n",
    "    return result_img, homography, matches_mask\n",
    "\n",
    "def visualize_keypoints(image_path):\n",
    "    \"\"\"\n",
    "    Visualize SIFT keypoints on an image\n",
    "    \n",
    "    Parameters:\n",
    "    image_path (str): Path to the image\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Image with keypoints drawn\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    sift = cv2.SIFT_create()\n",
    "    keypoints = sift.detect(gray, None)\n",
    "    \n",
    "    img_with_keypoints = cv2.drawKeypoints(gray, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    \n",
    "    return img_with_keypoints\n",
    "\n",
    "def visualize_matches(reference_image_path, query_image_path):\n",
    "    \"\"\"\n",
    "    Visualize matches between reference and query images\n",
    "    \n",
    "    Parameters:\n",
    "    reference_image_path (str): Path to the reference image\n",
    "    query_image_path (str): Path to the query image\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Image showing matches\n",
    "    \"\"\"\n",
    "    # Load images\n",
    "    reference_img = cv2.imread(reference_image_path)\n",
    "    query_img = cv2.imread(query_image_path)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    reference_gray = cv2.cvtColor(reference_img, cv2.COLOR_BGR2GRAY)\n",
    "    query_gray = cv2.cvtColor(query_img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Initialize SIFT detector\n",
    "    sift = cv2.SIFT_create()\n",
    "    \n",
    "    # Find keypoints and descriptors\n",
    "    kp_reference, des_reference = sift.detectAndCompute(reference_gray, None)\n",
    "    kp_query, des_query = sift.detectAndCompute(query_gray, None)\n",
    "    \n",
    "    # FLANN parameters\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "    \n",
    "    # Create FLANN matcher\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    \n",
    "    # Match descriptors\n",
    "    matches = flann.knnMatch(des_reference, des_query, k=2)\n",
    "    \n",
    "    # Apply ratio test\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            good_matches.append(m)\n",
    "    \n",
    "    # Draw matches\n",
    "    img_matches = cv2.drawMatches(reference_img, kp_reference, query_img, kp_query, good_matches, None,\n",
    "                                  flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    \n",
    "    return img_matches\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these with your actual image paths\n",
    "    reference_image = \"ref.png\"\n",
    "    query_image = \"shoe.jpg\"\n",
    "    \n",
    "    # Detect and match objects\n",
    "    result, homography, matches_mask = detect_and_match_objects(reference_image, query_image)\n",
    "    \n",
    "    # Convert result from BGR to RGB for displaying with matplotlib\n",
    "    result_rgb = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Visualize keypoints\n",
    "    reference_keypoints = visualize_keypoints(reference_image)\n",
    "    reference_keypoints_rgb = cv2.cvtColor(reference_keypoints, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Visualize matches\n",
    "    matches_visualization = visualize_matches(reference_image, query_image)\n",
    "    matches_visualization_rgb = cv2.cvtColor(matches_visualization, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Display results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(cv2.cvtColor(cv2.imread(reference_image), cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Reference Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.imshow(cv2.cvtColor(cv2.imread(query_image), cv2.COLOR_BGR2RGB))\n",
    "    plt.title('Query Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.imshow(reference_keypoints_rgb)\n",
    "    plt.title('SIFT Keypoints on Reference Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(result_rgb)\n",
    "    plt.title('Object Detection Result')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show matches in a separate figure\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(matches_visualization_rgb)\n",
    "    plt.title('Feature Matches')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ecdb3-7a1e-4ed0-8f89-4539563c0d55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
